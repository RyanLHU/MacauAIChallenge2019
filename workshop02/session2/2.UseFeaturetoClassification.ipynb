{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic sign classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Prepare the extracted feature\n",
    "### Last session you have been introduced a lot of features. These features all can be used for classification.\n",
    "It is certain that the pixel itself also can be regarded as a kind of feature.\n",
    "The tutorial of classification using raw pixel can be found [here](./GuideLine_For_ML_IJCAI_color.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Get Feature\n",
    "Here we just got a pre-extracted feature, the canny feature. \n",
    "Note that here we can change this \"canny feature\" to any features whatever you like :)\n",
    "\n",
    "We have already got the features with name \"CannyFeature.npy\", and the corresponding class files \"Label.txt\".\n",
    "So the next is to load the feature data and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the pre-extracted feature is: (359, 10000)\n",
      "The label is:\n",
      " [ 11  16  30  18  16  35  42  34 101 108 108 108 108 108 108 108  16  16\n",
      "  68  68  51  51 105  41  51  16  16  16  16 105 105  41 103  51  68  68\n",
      "  43  16  16  83  83 118 118 118 101 101  34  34  51  51 104  51  41  41\n",
      "  41 104  16  16  69  16  69 118  69  69  68  68  87  87  68  87  68  87\n",
      "  68  87  69  16  16  69  16  16  69  16  41  69  41  41  69  41  41 101\n",
      " 101 101 101 101  51  51  51  51  51 118 118 118 118 118  47  34  44  40\n",
      " 102 107  41 101  71  51  47  47  47  47  47  47  47  71  51  43  47  50\n",
      " 104  41  50  50  41  41  51  50  51 104  51  40  50  41  51  50  51  42\n",
      " 108  11  11  16  35  41  71  51  71  71  68  35  35 103 118 118 103  51\n",
      "  51  34  42  41  51  51 116  41  51  35  16  40  40 112  16  35  16 102\n",
      "  41 101  42  40  34 116  34  66  54  40  18 116  51 105  16  41 104  43\n",
      "  15  11  16 105 118  15  51  35  64  35  35  29  15  42  47  41 118 112\n",
      "  35  35 118 118  22  22  40 115 118 112  51  51 102  34  50  51  31  51\n",
      "  51  79  34  50 101  50 118 101  50  41  54  41  51  41 104  11  42  47\n",
      " 113 104 104  56  50 104 102  16  51  16  34  51  16  68  68  51  68  68\n",
      "  51  51  29 100  51  51  34  51  41  69  69 116  69 102  69  15  15  15\n",
      "  69  34  40  40  40  16 100 100  51  34 100 118  51  50  51  29 101  34\n",
      " 100  51  71 105  71 105  71  71  16  35 118  69 116 102  41 118 118  40\n",
      "  50  50  50 113 113 112 112 102 102 102 103  40 102 102  16  34 118 104\n",
      " 104 110 110 110  51  44  50 103  43  50  44  50  11  16 118 101  43]\n"
     ]
    }
   ],
   "source": [
    "# Load some packages \n",
    "import numpy as np\n",
    "# Load features\n",
    "CannyFeature = np.load(\"./CannyFeature.npy\")\n",
    "# Print the shape of the feature\n",
    "print (\"The shape of the pre-extracted feature is:\", CannyFeature.shape)\n",
    "Label = np.loadtxt(\"./label.txt\", dtype=np.int)\n",
    "print (\"The label is:\\n\", Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these features and label to train a classifier!\n",
    "\n",
    "### Building Classifier\n",
    "Here is the steps for the task:\n",
    "+ Standardize our data\n",
    "+ Split the data into training set and testing set\n",
    "+ Use the training set to train a model\n",
    "+ Evaluate the model\n",
    "\n",
    "First we preprocess our data, including standardizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the StandardScaler package\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the data\n",
    "DataScaler = StandardScaler().fit(CannyFeature)\n",
    "X_scaled = DataScaler.transform(CannyFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start the second step, split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train_test_split package\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the total data, 33%of which is regarded as the testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Label,\\\n",
    "                                                   test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a model. For example, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=20, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "rfclf = rf(n_estimators=500, max_features=20, random_state=42)\n",
    "\n",
    "rfclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the skills learned from the first workshop to search for better parameters of the Random Forest.\n",
    "\n",
    "For example, the grid search method with cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:   25.2s finished\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'n_estimators': [10, 20, 30, 50, 100, 200, 500, 1000], 'max_features': [10, 20, 50, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GridSearchCV model for optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Instantiate the random forest model\n",
    "rfclf = rf(random_state=42)\n",
    "# Set the parameters of the random forest for searching\n",
    "rfparams = {'n_estimators': [10,20,30,50,100,200,500,1000],\n",
    "            'max_features': [10, 20, 50, 100]}\n",
    "\n",
    "# Instantiate a grid search with cross validation model to optimize the random forest model with the parameters\n",
    "clf = GridSearchCV(rfclf, rfparams, n_jobs=-1, cv=5, verbose=1)\n",
    "# Use the training set to fit the model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the best estimators generated by searching to predict the sign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model params is {'max_features': 10, 'n_estimators': 500}\n",
      "The score of the best model is 0.26666666666666666\n",
      "The predicted label is [118 118 118  41  16  51  51  16  51  51  51  51 118  51  51  51  16  51\n",
      "  51  51  51  51  51  51  51  51  51  51  16  51  41  51  51  16  51  51\n",
      "  51  51  51  51  51  51  51  51  51  51  51  51  51  51 118  51  16 118\n",
      "  51  51 118  51 118  51  41  51  51  51  68  51 118  51  51  51  51  51\n",
      "  51  51  51  16 118  41  16  51  51  51  51  41  51  51  51  16  16  51\n",
      "  51  51  41  51  16  51 118  51  51 118  16  51  51  51  51  51  51 104\n",
      "  51  51  51  51  16  51  16  51 118  51  51]\n",
      "The accuracy for this model is 0.18487394957983194\n"
     ]
    }
   ],
   "source": [
    "# Show the best results\n",
    "print (\"The best model params is\", clf.best_params_)\n",
    "print (\"The score of the best model is\", clf.best_score_)\n",
    "\n",
    "# Get the best estimator\n",
    "bestclf = clf.best_estimator_\n",
    "# Use the best estimator to predict\n",
    "y_pred = bestclf.predict(X_test)\n",
    "print (\"The predicted label is\", y_pred)\n",
    "# Calculate the accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "print (\"The accuracy for this model is\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Conclusion\n",
    "In this part we learn how to use machine learning tools to classify the traffic sign. Next session you will reach the state-of-the-art techniques for the classification problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
